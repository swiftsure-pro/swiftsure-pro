<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en" xml:lang="en" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<head>
<META http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Concept: Key Measures of Test</title>
<meta name="uma.type" content="Concept">
<meta name="uma.name" content="key_measure_test">
<meta name="uma.presentationName" content="Key Measures of Test">
<meta name="element_type" content="concept">
<meta name="filetype" content="description">
<meta name="role" content="none">
<link rel="StyleSheet" href="./../../../css/default.css" type="text/css">
<script src="./../../../scripts/ContentPageResource.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageSection.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageSubSection.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageToolbar.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/contentPage.js" type="text/javascript" language="JavaScript"></script><script type="text/javascript" language="JavaScript">
					var backPath = './../../../';
					var imgPath = './../../../images/';
					var nodeInfo=null;
					contentPage.preload(imgPath, backPath, nodeInfo,  '', false, false, false);
				</script>
</head>
<body>
<div id="breadcrumbs"></div>
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr>
<td valign="top"><a name="Top"></a>
<div id="page-guid" value="_AhYiUHHVEdyzS55ez-koKA"></div>
<table border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td class="pageTitle" nowrap="true">Concept: Key Measures of Test</td><td width="100%">
<div align="right" id="contentPageToolbar"></div>
</td><td width="100%" class="expandCollapseLink" align="right"><a name="mainIndex" href="./../../../index.htm"></a><script language="JavaScript" type="text/javascript" src="./../../../scripts/treebrowser.js"></script></td>
</tr>
</table>
<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td class="pageTitleSeparator"><img src="./../../../images/shim.gif" alt="" title="" height="1"></td>
</tr>
</table>
<div class="overview">
<table width="97%" border="0" cellspacing="0" cellpadding="0">
<tr>
<td width="50"><img src="./../../../images/concept.gif" alt="" title=""></td><td>
<table class="overviewTable" border="0" cellspacing="0" cellpadding="0">
<tr>
<td valign="top">This guideline introduces coverage and quality measures for test suites.</td>
</tr>
</table>
</td>
</tr>
</table>
</div>
<div class="sectionHeading">Relationships</div>
<div class="sectionContent">
<table class="sectionTable" border="0" cellspacing="0" cellpadding="0">
<tr valign="top">
<th class="sectionTableHeading" scope="row">Related Elements</th><td class="sectionTableCell">
<ul>
<li>
<a href="./../../../core.tech.common.extend_supp/workproducts/test_case_335C5DEA.html" guid="_0ZS-0MlgEdmt3adZL5Dmdw">Test Case</a>
</li>
<li>
<a href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_evaluation_summary_7C2958B.html" guid="_iD7vkHFfEdy8Ac588DXPCQ">Test Evaluation Summary</a>
</li>
<li>
<a href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_ideas_list_195666E2.html" guid="_7ukFEHE7Edy8Ac588DXPCQ">Test Ideas List</a>
</li>
</ul>
</td>
</tr>
</table>
</div>
<div class="sectionHeading">Main Description</div>
<div class="sectionContent">
<table class="sectionTable" border="0" cellspacing="0" cellpadding="0">
<tr valign="top">
<td class="sectionTableSingleCell"><h3>
    <b><a id="Introduction" name="Introduction">Introduction</a></b>
</h3>
<p>
    The key measures of a test include coverage and quality.
</p>
<p>
    Test coverage is the measurement of testing completeness, and it's based on the coverage of testing expressed by the
    coverage of test requirements and test cases or by the coverage of executed code.
</p>
<p>
    Quality is a measure of the reliability, stability, and performance of the target-of-test (system or
    application-under-test). Quality is based on evaluating test results and analyzing change requests (defects) identified
    during testing.
</p>
<h3>
    <b><a id="Coverage" name="Coverage">Coverage Measures</a></b>
</h3>
<p>
    Coverage metrics provide answers to the question: "How complete is the testing?" The most commonly-used measures of
    coverage are based on the coverage of software requirements and source code. Basically, test coverage is any measure of
    completeness with respect to either a requirement (requirement-based), or the code's design and implementation criteria
    (code-based), such as verifying use cases (requirement-based) or executing all lines of code (code-based).
</p>
<p>
    Any systematic testing task is based on at least one test coverage strategy. The coverage strategy guides the design of
    test cases by stating the general purpose of the testing. The statement of coverage strategy can be as simple as
    verifying all performance.
</p>
<p>
    A requirements-based coverage strategy might be sufficient for yielding a quantifiable measure of testing completeness
    if the requirements are completely cataloged. For example, if all performance test requirements have been identified,
    then the test results can be referenced to get measures; for example, 75% of the performance test requirements have
    been verified.
</p>
<p>
    If code-based coverage is applied, test strategies are formulated in terms of how much of the source code has been
    executed by tests. This type of test coverage strategy is very important for safety-critical systems.
</p>
<p>
    Both measures can be derived manually (using the equations given in the next two headings) or may be calculated using
    test automation tools.
</p>
<h4>
    <a id="Requirements-based test coverage" name="Requirements-based test coverage">Requirements-based Test Coverage</a>
</h4>
<p>
    Requirements-based test coverage, measured several times during the test lifecycle, identifies the test coverage at a
    milestone in the testing lifecycle, such as the planned, implemented, executed, and successful test coverage.
</p>
<ul>
    <li>
        Test coverage is calculated using the following equation:
    </li>
</ul>
<blockquote>
    <p class="example">
        Test Coverage = T<sup><sup>(p,i,x,s)</sup></sup> / RfT<br />
        <br />
        Where:<br />
        T is the number of Tests (planned, implemented, executed, or successful), expressed as test procedures or test
        cases.
    </p>
    <p class="example">
        RfT is the total number of Requirements for Test.
    </p>
</blockquote>
<ul>
    <li>
        Planning: the test coverage is calculated to determine the planned test coverage in the following manner:
    </li>
</ul>
<blockquote>
    <p class="example">
        Test Coverage (planned) = T<sup><sup>p</sup></sup> / RfT<br />
        <br />
        Where:<br />
        T<sup><sup>p</sup></sup> is the number of planned Tests, expressed as test procedures or test cases.
    </p>
    <p class="example">
        RfT is the total number of Requirements for Test.
    </p>
</blockquote>
<ul>
    <li>
        Implementation: as test procedures are being implemented (as test scripts) test coverage is calculated using the
        following equation:
    </li>
</ul>
<blockquote>
    <p class="example">
        Test Coverage (implemented) = T<sup><sup>i</sup></sup> / RfT<br />
        <br />
        Where:<br />
        T<sup><sup>i</sup></sup> is the number of Tests implemented, expressed by the number of test procedures or test
        cases for which there are corresponding test scripts.
    </p>
    <p class="example">
        RfT is the total number of Requirements for Test.
    </p>
</blockquote>
<ul>
    <li>
        Execution:&nbsp;there are two test coverage measures used-one identifies the test coverage achieved by executing
        the tests and the second identifies the successful test coverage (those tests that executed without failures, such
        as defects or unexpected results). 
        <p>
            These coverage measures are calculated using the following equations:
        </p>
        <blockquote>
            <p class="example">
                Test Coverage (executed) = T<sup><sup>x</sup></sup> / RfT
            </p>
        </blockquote>
        <blockquote>
            <p class="example">
                Where:<br />
                T<sup><sup>x</sup></sup> is the number of Tests executed, expressed as test procedures or test cases.
            </p>
        </blockquote>
        <blockquote>
            <p class="example">
                RfT is the total number of Requirements for Test.
            </p>
        </blockquote>
    </li>
    <li style="LIST-STYLE-TYPE: none">
        Successful Test Coverage (executed) = T<sup><sup>s</sup></sup> / RfT
    </li>
</ul>
<blockquote>
    <blockquote>
        <p class="example">
            Where:<br />
            T<sup><sup>s</sup></sup> is the number of Tests executed, expressed as test procedures or test cases that
            completed successfully, without defects.
        </p>
    </blockquote>
    <blockquote>
        <p class="example">
            RfT is the total number of Requirements for Test.
        </p>
    </blockquote>
</blockquote>
<div style="MARGIN-LEFT: 4em">
    <br />
    <br />
</div>
<p>
    Turning the above ratios into percentages allows for the following statement of requirements-based test coverage:
</p>
<blockquote>
    <p>
        x% of test cases (T<sup><sup>(p,i,x,s)</sup></sup> in the above equations) have been covered with a success rate of
        y%
    </p>
</blockquote>
<p>
    This meaningful statement of test coverage can be matched against a defined success criteria. If the criteria have not
    been met, then the statement provides a basis for predicting how much testing effort remains.
</p>
<h4>
    <a id="Code-based test coverage" name="Code-based test coverage">Code-based Test Coverage</a>
</h4>
<p>
    Code-based test coverage measures how much code has been executed during the test, compared to how much code is left to
    execute. Code coverage can be based on control flows (statement, branch, or paths) or data flows.
</p>
<ul>
    <li>
        In control-flow coverage, the aim is to test lines of code, branch conditions, paths through the code, or other
        elements of the software's flow of control.
    </li>
    <li>
        In data-flow coverage, the aim is to test that data states remain valid through the operation of the software; for
        example, that a data element is defined before it's used.
    </li>
</ul>
<p>
    Code-based test coverage is calculated by the following equation:
</p>
<blockquote>
    <p class="example">
        Test Coverage = I<sup><sup>e</sup></sup> / TIic
    </p>
</blockquote>
<blockquote>
    <p class="example">
        Where:<br />
        I<sup><sup>e</sup></sup> is the number of items executed, expressed as code statements, code branches, code paths,
        data state decision points, or data element names.
    </p>
</blockquote>
<blockquote>
    <p class="example">
        TIic is the total number of items in the code.
    </p>
</blockquote><br />
<br />
<p>
    Turning this ratio into a percentage allows the following statement of code-based test coverage:
</p>
<blockquote>
    <p>
        x% of test cases (I in the above equation) have been covered with a success rate of y%
    </p>
</blockquote>
<p>
    This meaningful statement of test coverage can be matched against a defined success criteria. If the criteria have not
    been met, then the statement provides a basis for predicting how much testing effort remains.
</p>
<h3>
    <b><a id="Quality" name="Quality">Measuring Perceived Quality</a></b>
</h3>
<p>
    Although evaluating test coverage provides a measure of the extent of completeness of the testing effort, evaluating
    defects discovered during testing provides the best indication of the software quality as it has been experienced. This
    perception of quality can be used to reason about the general quality of the software system as a whole. Perceived
    Software Quality is a measure of how well the software meets the requirements levied on it, therefore, in this context,
    defects are considered as a type of change request in which the target-of-test failed to meet the software
    requirements.
</p>
<p>
    Defect evaluation could be based on methods that range from simple defect counts to rigorous statistical modeling.
</p>
<p>
    Rigorous evaluation uses assumptions about the arrival or discovery rates of defects during the testing process. A
    common model assumes that the rate follows a Poisson distribution. The actual data about defect rates are then fit to
    the model. The resulting evaluation estimates the current software reliability and predicts how the reliability will
    grow if testing and defect removal continue. This evaluation is described as software-reliability growth modeling and
    it's an area of active study. Due to the lack of tool support for this type of evaluation, you want to carefully
    balance the cost of using this approach with the benefits gained.
</p>
<p>
    <b>Defects analysis</b> involves analyzing the distribution of defects over the values of one or more of the attributes
    associated with a defect. Defect analysis provides an indication of the reliability of the software.
</p>
<p>
    In defect analysis, four main defect attributes are commonly analyzed:
</p>
<ul>
    <li>
        <b>Status</b> - the current state of the defect (open, being fixed, closed, and so forth).
    </li>
    <li>
        <b>Priority</b> - the relative importance of this defect being addressed and resolved.
    </li>
    <li>
        <b>Severity</b> - the relative impact of this defect to the user, an organization, third parties, and so on.
    </li>
    <li>
        <b>Source</b> - where and what is the originating fault that results in this defect or what component will be fixed
        to eliminate this defect.
    </li>
</ul>
<p>
    Defect counts can be reported as a function of time, creating a Defect Trend diagram or report. They can also be
    reported in a Defect Density Report as a function of one or more defect attributes, like severity or status. These
    types of analysis provide a perspective on the trends or on the distribution of defects that reveal the software's
    reliability.<br />
    <br />
    For example, it's expected that defect discovery rates will eventually diminish as the testing and fixing progresses. A
    defect or poor quality threshold can be established at which point the software quality will be unacceptable. Defect
    counts can also be reported based on the origin in the Implementation model, allowing for detection of "weak modules",
    "hot spots", and parts of the software that keep being fixed again and again, which indicates more fundamental design
    flaws.
</p>
<p>
    Only confirmed defects are included in an analysis of this kind. Not all reported defects denote an actual flaw; some
    might be enhancement requests outside of the project's scope, or may describe a defect that's already been reported.
    However, it's valuable to look at and analyze why many defects, which are either duplicates or not confirmed defects,
    are being reported.
</p>
<h4>
    <a id="Defect Reports" name="Defect Reports">Defect Reports</a>
</h4>
<p>
    The defect evaluation should be performed&nbsp;based on multiple reporting categories, as follows:
</p>
<ul>
    <li>
        Defect Distribution (Density) Reports allow defect counts to be shown as a function of one or two defect
        attributes.
    </li>
    <li>
        Defect Age Reports are a special type of defect distribution report. Defect age reports show how long a defect has
        been in a particular state, such as Open. In any age category, defects can also be sorted by another attribute,
        such as Owner.
    </li>
    <li>
        Defect Trend Reports show defect counts, by status (new, open, or closed), as a function of time. The trend reports
        can be cumulative or non-cumulative.
    </li>
</ul>
<p>
    Many of these reports are valuable in assessing software quality. They are most useful when analyzed in conjunction
    with Test results and progress reports that show the results of the tests conducted over a number of iterations and
    test cycles for the application-under-test. The usual test criteria include a statement about the tolerable numbers of
    open defects in particular categories, such as severity class, which is easily checked with an evaluation of defect
    distribution. By sorting or grouping this distribution by test motivators, the evaluation can be focused on important
    areas of concern.
</p>
<p>
    Normally tool support is required to effectively produce reports of this kind.
</p>
<h4>
    <b><a id="Defect density reports:" name="Defect density reports:">Defect Density Reports</a></b>
</h4>
<h5>
    <b>Defect status versus priority</b>
</h5>
<p>
    Give each defect a priority. It's usually practical and sufficient to have four levels of priority, such as:
</p>
<ul>
    <li>
        Urgent priority (resolve immediately)
    </li>
    <li>
        High priority
    </li>
    <li>
        Normal priority
    </li>
    <li>
        Low priority
    </li>
</ul>
<p>
    <b>Note</b>: Criteria for a successful test could be expressed in terms of how the distribution of defects over these
    priority levels should look. For example, successful test criteria might be "no Priority 1 defects and fewer than five
    Priority 2 defects are open". A defect distribution diagram, such as the following, should be generated.
</p>
<p align="center">
    <img height="233" alt="Defect Distribution Diagram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas1.gif" width="378" />
</p><br />
<br />
<p>
    It's clear that the criteria has not been met. This diagram needs to include a filter to show only open defects, as
    required by the test criteria.
</p>
<h5>
    <b>Defect status versus severity</b>
</h5>
<p>
    Defect Severity Reports show how many defects there are for each severity class; for example, fatal error, major
    function not performed, minor annoyance.
</p>
<h5>
    <b>Defect status versus location in the Implementation model</b>
</h5>
<p>
    Defect Source Reports show distribution of defects on elements in the Implementation model.
</p>
<h4>
    <b><a id="Defect aging reports:" name="Defect aging reports:">Defect Aging Reports</a></b>
</h4>
<p>
    Defect Age Analysis provides good feedback on the effectiveness of the testing and the defect removal tasks. For
    example, if the majority of older, unresolved defects are in a pending-validation state, it probably means that not
    enough resources are applied to the retesting effort.
</p>
<h4>
    <b><a id="Defect trend reports:" name="Defect trend reports:">Defect Trend Reports</a></b>
</h4>
<p>
    Defect Trend Reports identify defect rates and provide a particularly good view of the state of the testing. Defect
    trends follow a fairly predictable pattern in a testing cycle. Early in the cycle, the defect rates rise quickly, then
    they reach a peak, and decrease at a slower rate over time.
</p><br />
<br />
<p align="center">
    <img height="230" alt="Defect Trend Reports Diagram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas2.gif" width="373" />
</p>
<p>
    To find problems, the project schedule can be reviewed in light of this trend. For example, if the defect rates are
    still rising in the third week of a four-week test cycle, the project is clearly not on schedule.
</p>
<p>
    This simple trend analysis assumes that defects are being fixed promptly and that the fixes are being tested in
    subsequent builds, so that the rate of closing defects should follow the same profile as the rate of finding defects.
    When this does not happen, it indicates a problem with the defect-resolution process; the defect fixing resources or
    the resources to retest and validate fixes could be inadequate.
</p>
<p align="center">
    <img height="230" alt="Trend Analysis Report Diagram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas3.gif" width="469" />
</p>
<p>
    The trend reflected in this report shows that new defects are discovered and opened quickly at the beginning of the
    project, and that they decrease over time. The trend for open defects is similar to that for new defects, but lags
    slightly behind. The trend for closing defects increases over time as open defects are fixed and verified. These trends
    depict a successful effort.
</p>
<p>
    If your trends deviate dramatically from these, they may indicate a problem and identify when additional resources need
    to be applied to specific areas of development or testing.
</p>
<p>
    When combined with the measures of test coverage, the defect analysis provides a very good assessment on which to base
    the test completion criteria.
</p>
<h3>
    <a id="Performance" name="Performance">Performance Measures</a>
</h3>
<p>
    Several measures are used for assessing the performance behaviors of the target-of-test and for focusing on capturing
    data related to behaviors such as response time, timing profiles, execution flow, operational reliability, and limits.
</p>
<p>
    The primary performance measures include:
</p>
<ul>
    <li>
        <b>Dynamic Monitoring</b> - real-time capture and display of the status and state of each test script being
        executed during the test execution.
    </li>
    <li>
        <b>Response Time and Throughput Reports</b> - measurement of the response times and throughput of the
        target-of-test for specified actors and use cases.
    </li>
    <li>
        <b>Percentile Reports</b> - percentile measurement and calculation of the data collected values.
    </li>
    <li>
        <b>Comparison Reports</b> - differences or trends between two (or more) sets of data representing different test
        executions.
    </li>
    <li>
        <b>Trace Reports</b> - details of the messages and conversations between the actor (test script) and the
        target-of-test.
    </li>
</ul>
<h4>
    <a id="Dynamic Monitoring" name="Dynamic Monitoring">Dynamic Monitoring</a>
</h4>
<p>
    Dynamic monitoring provides real-time display and reporting during test execution, typically in the form of a histogram
    or a graph. The report monitors or assesses performance test execution by displaying the current state, status, and
    progress of the test scripts.
</p>
<p align="center">
    <img height="333" alt="Dynamic Monitoring Displayed as a Histogram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas4.gif" width="501" />
</p>
<p>
    For example, in the preceding histogram, there are 80 test scripts executing the same use case. In this graph, 14 test
    scripts are in the Idle state, 12 in the Query, 34 in SQL Execution, 4 in SQL Connect, and 16 in the Other state. As
    the test progresses, you would expect to see the number of scripts in each state change. The displayed output would be
    typical of a test execution that is executing normally and is in the middle of its execution. However, if test scripts
    remain in one state or do not show changes during test execution, this could indicate a problem with the test
    execution, or the need to implement or evaluate other performance measures.
</p>
<h4>
    <a id="Response time and throughput reports" name="Response time and throughput reports">Response Time and Throughput
    Reports</a>
</h4>
<p>
    Response Time and Throughput Reports, as their name implies, measure and calculate the performance behaviors related to
    time and throughput (number of transactions processed). Typically, these reports are displayed as a graph with response
    time (or number of transactions) on the "y" axis and events on the "x" axis.
</p>
<p align="center">
    <img height="333" alt="Sample Throughput and Analysis Report Diagram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas5.gif" width="499" />
</p>
<p>
    It's often valuable to calculate and display statistical information, such as the mean and standard deviation of the
    data values in addition to showing the actual performance behaviors.
</p>
<h4>
    <a id="Percentile Reports" name="Percentile Reports">Percentile Reports</a>
</h4>
<p>
    Percentile Reports provide another statistical calculation of performance by displaying population percentile values
    for data types collected.
</p>
<p align="center">
    <img height="333" alt="Sample Percentile Report Diagram" src="./../../../core.tech.common.extend_supp-ibm/guidances/concepts/./resources/keymeas6.gif" width="499" />
</p>
<h4>
    <a id="Comparison Reports" name="Comparison Reports">Comparison Reports</a>
</h4>
<p>
    It's important to compare the results of one performance test execution with that of another, so you can evaluate the
    impact of changes made between test executions on the performance behaviors. Use Comparison Reports to display the
    difference between two sets of data (each representing different test executions) or trends between many executions of
    test.
</p>
<h4>
    <a id="Trace and Profile Reports" name="Trace and Profile Reports">Trace and Profile Reports</a>
</h4>
<p>
    When performance behaviors are unacceptable or when performance monitoring indicates possible bottlenecks (such as when
    test scripts remain in a given state for exceedingly long periods), trace reporting could be the most valuable report.
    Trace and Profile Reports display lower-level information. This information includes the messages between the actor and
    the target-of-test, execution flow, data access, and the function and system calls.
</p><br />
<br /></td>
</tr>
</table>
</div>
<table class="copyright" border="0" cellspacing="0" cellpadding="0">
<tr>
<td class="copyright"><p>
    Licensed Materials - Property of IBM<br />
    &copy; &nbsp;Copyright IBM Corp.&nbsp;1987, 2011.&nbsp; All Rights Reserved.
</p></td>
</tr>
</table>
</td>
</tr>
</table>
</body>
<script type="text/javascript" language="JavaScript">
				contentPage.onload();
			</script>
</html>
