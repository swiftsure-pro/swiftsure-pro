<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en" xml:lang="en" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
<head>
<META http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Roadmap: How to Adopt the Independent Testing Practice</title>
<meta name="uma.type" content="Roadmap">
<meta name="uma.name" content="how_to_adopt_structured_testing">
<meta name="uma.presentationName" content="How to Adopt the Independent Testing Practice">
<meta name="element_type" content="other">
<meta name="filetype" content="description">
<meta name="role" content="none">
<link rel="StyleSheet" href="./../../../css/default.css" type="text/css">
<script src="./../../../scripts/ContentPageResource.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageSection.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageSubSection.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/ContentPageToolbar.js" type="text/javascript" language="JavaScript"></script><script src="./../../../scripts/contentPage.js" type="text/javascript" language="JavaScript"></script><script type="text/javascript" language="JavaScript">
					var backPath = './../../../';
					var imgPath = './../../../images/';
					var nodeInfo=null;
					contentPage.preload(imgPath, backPath, nodeInfo,  '', false, false, false);
				</script>
</head>
<body>
<div id="breadcrumbs"></div>
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr>
<td valign="top"><a name="Top"></a>
<div id="page-guid" value="_5bS8YNUcEdySMfcrDUmpxg"></div>
<table border="0" cellspacing="0" cellpadding="0" width="100%">
<tr>
<td class="pageTitle" nowrap="true">Roadmap: How to Adopt the Independent Testing Practice</td><td width="100%">
<div align="right" id="contentPageToolbar"></div>
</td><td width="100%" class="expandCollapseLink" align="right"><a name="mainIndex" href="./../../../index.htm"></a><script language="JavaScript" type="text/javascript" src="./../../../scripts/treebrowser.js"></script></td>
</tr>
</table>
<table width="100%" border="0" cellpadding="0" cellspacing="0">
<tr>
<td class="pageTitleSeparator"><img src="./../../../images/shim.gif" alt="" title="" height="1"></td>
</tr>
</table>
<div class="overview">
<table width="97%" border="0" cellspacing="0" cellpadding="0">
<tr>
<td width="50"><img src="./../../../images/roadmap.gif" alt="" title=""></td><td>
<table class="overviewTable" border="0" cellspacing="0" cellpadding="0">
<tr>
<td valign="top">This roadmap describes how to adopt the Independent Testing practice.</td>
</tr>
</table>
</td>
</tr>
</table>
</div>
<div class="sectionHeading">Main Description</div>
<div class="sectionContent">
<table class="sectionTable" border="0" cellspacing="0" cellpadding="0">
<tr valign="top">
<td class="sectionTableSingleCell"><p>
    Testing focuses primarily on evaluating or assessing product quality. These are the main activities:
</p>
<ul>
    <li>
        Find and document defects in software quality.
    </li>
    <li>
        Advise on the perceived software quality.
    </li>
    <li>
        Validate and prove the assumptions made in design and requirement specifications through demonstration.
    </li>
    <li>
        Validate that the software product works as designed.
    </li>
    <li>
        Validate that the requirements are implemented appropriately.
    </li>
</ul>
<p>
    Testing is to find and expose weaknesses in the software product. To get the biggest benefit, you need a different
    philosophy than what's used in the rest of the development cycle. A somewhat subtle difference is that&nbsp;during most
    of the development the&nbsp;focus is&nbsp;on completeness, whereas testing focuses on incompleteness. A good test
    effort is driven by questions such as "How could this software 'break,' and&nbsp;in&nbsp;what possible situations could
    this software fail to work predictably?"
</p>
<p>
    Testing challenges the assumptions, risks, and uncertainty inherent in the work of developing a product and addresses
    those concerns by using concrete demonstrations and impartial evaluations. You need to avoid two potential extremes:
</p>
<ul>
    <li>
        An approach that does not suitably or effectively challenge the software and expose its inherent problems or
        weaknesses
    </li>
    <li>
        An approach that is inappropriately negative (because you might find it impossible to consider the software product
        of acceptable quality and could alienate the rest of the development team from the test effort)
    </li>
</ul>
<p>
    Information presented in various surveys and essays states that software testing accounts for 30 to 50 percent of total
    software development costs.&nbsp; It is somewhat surprising that most people believe computer software is not
    well-tested before it is delivered. This contradiction is rooted in a few key issues:
</p>
<ul>
    <li>
        Testing software is very difficult. How do you quantify the different ways in which a given program can behave?
    </li>
    <li>
        Typically, testing is done without a clear methodology, thus creating results that vary from project to project and
        from organization to organization. Success is primarily a factor of the quality and skills of the individuals.
    </li>
    <li>
        Productivity tools are used insufficiently, which makes the laborious aspects of testing unmanageable. In addition
        to the lack of automated test execution, many test efforts are conducted without tools that let you effectively
        manage extensive test data and test results. Flexibility of use and complexity of software make complete testing an
        impossible goal. Using a well-conceived methodology and state-of-the-art tools can improve both the productivity
        and effectiveness of software testing.
    </li>
</ul>
<h3>
    How to adopt this practice
</h3>
<p>
    Here's one possible scenario for adopting this practice. You may want to add, change, or&nbsp;remove steps to design an
    adoption roadmap more suitable to your environment. Hiring a consultant who is experienced in&nbsp;this area&nbsp;will
    also speed your adoption of the practice and help avoid common pitfalls.
</p>
<ol>
    <li>
        Educate the team about the&nbsp;Independent Testing practice. Courses and presentations are available (see the
        Additional Information section in <a class="elementLink" href="./../../../practice.tech.independent_testing.base-ibm/guidances/practices/independent_testing_30ABFB15.html" guid="_fnn70CFAEd2KZfP7K7AmSg">Independent Testing</a>).
    </li>
    <li>
        Have the team review the material in this practice.
    </li>
    <li>
        Perform a gap analysis between your current practices and the proposed one. Focus on problem areas. Try to
        distinguish between real differences and just terminology mismatches.
    </li>
    <li>
        Identify extension points and extend this practice to reflect any important requirements and constraints in your
        organization.
    </li>
    <li>
        Reuse the current elements that reflect your specific environment, such as templates and examples, by attaching
        them to the proposed practice.
    </li>
    <li>
        Identify and prepare to collect the information or metrics that will tell you how well you're adopting this
        practice. Make sure that the data and metrics are easy to collect. Highly accurate metrics that are difficult to
        collect are often abandoned, thus they provide no value. Coarser measurements that are easy to collect usually
        provide sufficient information, and it's more likely that they'll continue to be collected.
    </li>
    <li>
        Develop an adoption plan with specific goals for each step. An iterative, incremental approach works best. Try to
        tackle the problem points identified earlier.
    </li>
    <li>
        Select a project where you will start applying the new practice. This pilot project should be sufficiently visible
        and risky to properly adopt this practice.
    </li>
    <li>
        Evaluate your adoption based on the objectives and metrics that you defined.
    </li>
    <li>
        Make adjustments based on your evaluation. Eliminate tools or tool features that don't prove effective, and
        increase practices that are efficient and improve quality.
    </li>
    <li>
        Determine the next step in adoption.
    </li>
    <li>
        Continue to extend or modify this practice to reflect how your team and organization is performing this new process
        and what the next increment of adoption should be for your team.
    </li>
</ol>
<h3>
    How to tailor this practice&nbsp;
</h3>
<p>
    This section presents the most important aspects that you need to consider when you tailor this practice to meet your
    specific environment.
</p>
<h4>
    Decide how to use work products
</h4>
<p>
    Make a decision about what work products are to be&nbsp;used and how they are to be used.&nbsp; It is also important to
    tailor each work product to be used to fit the needs of the project.&nbsp;
</p>
<p>
    The table that follows specifies which testing work products are recommended and which are considered optional (can be
    used&nbsp;in certain cases). For additional tailoring considerations, see the Tailoring section of the work product
    description page.
</p>
<div>
    <table     style="BORDER-BOTTOM: rgb(128,128,128) 1px solid; BORDER-LEFT: rgb(128,128,128) 1px solid; BORDER-TOP: rgb(128,128,128) 1px solid; BORDER-RIGHT: rgb(128,128,128) 1px solid"      border="1" cellspacing="0" bordercolorlight="#808080" bordercolordark="#808080" cellpadding="4" width="100%">
        <tbody>
            <tr>
                <th width="20%" scope="col" align="left">
                    Work product
                </th>
                <th width="40%" scope="col" align="left">
                    Purpose
                </th>
                <th width="40%" scope="col" align="left">
                    Tailoring (optional, recommended)
                </th>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_evaluation_summary_7C2958B.html" guid="_iD7vkHFfEdy8Ac588DXPCQ">Test Evaluation Summary</a>&nbsp;
                </td>
                <td valign="top" width="40%">
                    Summarizes the test results for use primarily by the management team and other stakeholders external to
                    the test team.
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    Where the project culture is relatively information, it may be appropriate simply to record test
                    results and not create formal evaluation summaries. In other cases, Test Evaluation Summaries can be
                    included as a section within other assessment work products, such as the&nbsp;Iteration Assessment or
                    Review Record.&nbsp;
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_findings_565B9E24.html" guid="_DwNAQHE8Edy8Ac588DXPCQ">Test Findings</a>
                </td>
                <td valign="top" width="40%">
                    This work product is the analyzed result determined from the raw data in one or more test logs.
                </td>
                <td width="40%">
                    Recommended. Most test teams retain some form of reasonably detailed record of the results of testing.
                    Manual testing results are usually recorded directly here and combined with the distilled test logs
                    from automated tests.<br />
                    <br />
                    In some cases, test teams will go directly from the test logs to producing the Test Evaluation Summary.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp/workproducts/test_log_CBA2FDF4.html" guid="_0ZlSsMlgEdmt3adZL5Dmdw">Test Log</a>&nbsp;
                </td>
                <td valign="top" width="40%">
                    <p>
                        The raw data output during test execution, typically produced by automated tests.
                    </p>
                </td>
                <td width="40%">
                    <p>
                        Optional.
                    </p>
                    <p>
                        Many projects that perform automated testing will have some form of test log. Where projects differ
                        is whether the test logs are retained or discarded after the test results have been determined.
                    </p>
                    <p>
                        You might retain test logs if you need to satisfy certain audit requirements, if you want to
                        analyze how the raw test output data changes over time, or if you are uncertain at the outset of
                        all the analysis that you may be required to provide.
                    </p>
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_suite_DA2E3AF2.html" guid="_roUk0HE8Edy8Ac588DXPCQ">Test Suite</a>
                </td>
                <td valign="top" width="40%">
                    Used to group individual related tests (test scripts) together in meaningful subsets.
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    Also required to define any test script execution sequences that are required for tests to work
                    correctly.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_ideas_list_195666E2.html" guid="_7ukFEHE7Edy8Ac588DXPCQ">Test Ideas List</a>&nbsp;
                </td>
                <td valign="top" width="40%">
                    This is an enumerated list of tests to consider conducting (often partially formed).
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    In some cases these lists will be informally defined and discarded after the test scripts or test cases
                    have been defined from them.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_strategy_AB15461A.html" guid="_nbNrMHE8Edy8Ac588DXPCQ">Test Strategy</a>
                </td>
                <td valign="top" width="40%">
                    Defines the strategic plan for how the test effort will be conducted against one or more aspects of the
                    target system.
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    A single test strategy per project or per phase within a project is recommended in most cases.
                    Optionally, you might reuse existing strategies where appropriate, or you might further subdivide the
                    test strategies based on the type of testing that you are conducting.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLinkWithUserText" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_plan_7503019A.html" guid="_yiMRwHFfEdy8Ac588DXPCQ">Iteration Test Plan</a>
                </td>
                <td valign="top" width="40%">
                    Defines finer-grained testing goals, objectives, motivations, approach, resources, schedule and work
                    products that govern an iteration.
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    A separate test plan for each iteration is best to define the specific, fine-grained test strategy.
                    Optionally, you can include the test plan as a section within the iteration plan.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLinkWithUserText" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_plan_7503019A.html" guid="_yiMRwHFfEdy8Ac588DXPCQ">Master Test Plan</a>
                </td>
                <td valign="top" width="40%">
                    Defines high-level testing goals, objectives, approaches, resources, schedule, and work products that
                    govern a phase or the entire lifecycle.
                </td>
                <td width="40%">
                    Optional. Useful for most projects.<br />
                    <br />
                    A Master Test Plan defines the high-level strategy for the test effort over large parts of the software
                    development lifecycle. Optionally, you can include the Test Plan as a section within a Software
                    Development Plan.<br />
                    <br />
                    Consider whether to maintain a Master Test Plan in addition to the Iteration Test Plans. The Master
                    Test Plan covers mainly logistical and process information that typically relates to the entire project
                    lifecycle; it is unlikely to change between iterations.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp/workproducts/test_script_39A30BA2.html" guid="_0ZfMEMlgEdmt3adZL5Dmdw">Test Script</a>, <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_data_5B6611FC.html" guid="_3pyoEHE7Edy8Ac588DXPCQ">Test Data</a>
                </td>
                <td valign="top" width="40%">
                    The test scripts and test data are the realization or implementation of the test, where the test script
                    embodies the procedural aspects and the test data embodies the defining characteristics.
                </td>
                <td width="40%">
                    Recommended for most projects.<br />
                    <br />
                    Where projects differ is how formally these work products are treated. In some cases, these are
                    informal and transitory, and the test team is judged based on other criteria. In other cases --
                    especially with automated tests -- the test scripts and associated test data (or some subset thereof)
                    are regarded as major deliverables of the test effort.
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp/workproducts/test_case_335C5DEA.html" guid="_0ZS-0MlgEdmt3adZL5Dmdw">Test Case</a>
                </td>
                <td valign="top" width="40%">
                    <p>
                        Defines a specific set of test inputs, execution conditions, and expected results.
                    </p>
                    <p>
                        Documenting test cases allows them to be reviewed for completeness and correctness, and considered
                        before implementation effort is planned and expended.
                    </p>
                    <p>
                        This is most useful where the input, execution conditions, and expected results are particularly
                        complex.
                    </p>
                </td>
                <td width="40%">
                    <p>
                        On most projects, were the conditions required to conduct a specific test are complex or extensive,
                        it is advisable to define test cases. You will also need to document test cases where they are a
                        contractually required deliverable.
                    </p>
                    <p>
                        In most other cases, it is more useful to maintain the Test Ideas List and the Implemented Test
                        Scripts List rather than detailed descriptions of test cases.
                    </p>
                    <p>
                        Some projects will simply outline test cases at a high level and defer details to the test scripts.
                        Another style commonly used is to document the test case information as comments within the test
                        scripts.
                    </p>
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    Workload Specification
                </td>
                <td valign="top" width="40%">
                    <p>
                        A specialized type of test case. Used to define a representative workload to allow quality risks
                        associated with the system operating under load to be assessed.
                    </p>
                </td>
                <td width="40%">
                    <p>
                        Recommended for most systems, especially those where system performance under load must be
                        evaluated or where there are other significant quality risks associated with system operation under
                        load.
                    </p>
                    <p>
                        Not usually required for systems that will be deployed on a standalone target system.
                    </p>
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <p>
                        Testability classes&nbsp;in the Design model
                    </p>
                    <p>
                        Testability elements in the Implementation model
                    </p>
                </td>
                <td valign="top" width="40%">
                    <p>
                        If the project has to develop significant additional specialized behavior to accommodate and
                        support testing, these concerns are represented by the inclusion of testability classes in the
                        Design model and the testability elements in the Implementation model.
                    </p>
                </td>
                <td valign="top" width="40%">
                    <p>
                        Where required.
                    </p>
                    <p>
                        Stubs are a common category of test classes and test components.
                    </p>
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_architecture_2AEFCE20.html" guid="_QGQ-AHE8Edy8Ac588DXPCQ">Test Architecture</a>
                </td>
                <td valign="top" width="40%">
                    <p>
                        Provides an architectural overview of the test automation system by using several different
                        architectural views to depict different aspects of the system.
                    </p>
                </td>
                <td width="40%">
                    <p>
                        Optional.
                    </p>
                    <p>
                        Recommended on projects where the test architecture is relatively complex, when a large number of
                        staff members will be collaborating on building automated tests, or when the test automation system
                        is expected to be maintained over a long period of time.
                    </p>
                    <p>
                        In some cases, this might simply be a whiteboard diagram that is recorded centrally for interested
                        parties to consult.
                    </p>
                </td>
            </tr>
            <tr>
                <td valign="top" width="20%">
                    <a class="elementLink" href="./../../../core.tech.common.extend_supp-ibm/workproducts/test_interface_spec_A9177D18.html" guid="_jFAf8HE8Edy8Ac588DXPCQ">Test Interface Specification</a>
                </td>
                <td valign="top" width="40%">
                    <p>
                        Defines a required set of behaviors by a classifier (specifically, a class, subsystem or component)
                        for the purposes of testing (testability). Common types include test access, stubbed behavior,
                        diagnostic logging, and test oracles.
                    </p>
                </td>
                <td width="40%">
                    <p>
                        Optional.
                    </p>
                    <p>
                        On many projects, there is either sufficient accessibility for test in the visible operations on
                        classes, user interfaces etc.
                    </p>
                    <p>
                        Common reasons to create test interface specifications include UI extensions to allow GUI test
                        tools to interact with the tool and diagnostic message logging routines, especially for batch
                        processes.
                    </p>
                </td>
            </tr>
        </tbody>
    </table>
</div>
<h4>
    <br />
    Decide how to review work products
</h4>
<ul>
    <li>
        <b>Defects:</b> The treatment of Defect reviews is very much dependent on context. However, they are generally
        treated as <i>Informal</i>, <i>Formal-Internal</i>, or <i>Formal-External</i>. This review process is often
        enforced or at least assisted by workflow management in a defect-tracking system. In general, the level of review
        formality often relates to the perceived severity or impact of the defect, but factors such as project culture and
        level of ceremony often have an effect on the choice of review handling methods. 
        <p>
            In some cases, you may need to consider separating the handling of defects (also known as symptoms or failures)
            from faults: the actual source of the error. For small projects, you can typically manage by tracking only the
            defects and implicitly handle the faults. However, as the system grows in complexity, it may be beneficial to
            separate the management of defects from faults. For example, several defects may be caused by the same fault.
            If a fault is fixed, it's necessary to find the reported defects and inform those users who submitted the
            defects, which is only possible if defects and faults can be identified separately.
        </p>
    </li>
    <li>
        <b>Test plan and test strategy:</b> In any project where the testing is nontrivial, you will need some form of test
        plan or strategy. Generally you'll need a test plan for each iteration and some form of governing test strategy.
        Optionally, you might create and maintain a Master Test Plan. In many cases, these work products are reviewed as
        Informal; that is, they are reviewed but not formally approved. Where testing visibility is important to
        stakeholders external to the test team, it should be treated as Formal-Internal or even Formal-External.
    </li>
</ul>
<ul>
    <li>
        <b>Test scripts:</b> Test scripts are usually treated as <b>Informal</b>; that is, they are approved by someone
        within the test team. If the test scripts are to be used by many testers and shared or reused for many different
        tests, they should be treated as Formal-Internal.
    </li>
</ul>
<ul>
    <li>
        <b>Test cases:</b> Test cases are created by the test team and, depending on context, are typically reviewed using
        either an Informal process or simply not reviewed as all. Where appropriate, test cases might be approved by other
        team members, in which case they can be treated as Formal-Internal, or they can be reviewed by external
        stakeholders, in which case they would be Formal-External. 
        <p>
            As a general heuristic, we recommend that you plan to formally review only the test cases that it is necessary
            to review, which generally will be limited to a small subset that represents the most significant test cases.
            For example, where a customer wants to validate a product before it is released, a subset of the test cases
            could be selected as the basis for that validation. These test cases should be treated as Formal-External.
        </p>
    </li>
    <li>
        <b>Test work products in design and implementation.</b> <i>Testability classes</i> are found in the Design model,
        and <i>testability elements</i> are in the Implementation model. There are also two other related work products
        (although not specific to tests): <i>packages</i> in the Design model, and <i>subsystems</i> in the Implementation
        model. 
        <p>
            These work products are design and implementation work products. However, they're created for the purpose of
            supporting testing functionality in the software. The natural place to keep them is with the design and
            implementation work products. Remember to name or otherwise label them in such a way that they are clearly
            separated from the design and implementation of the core system. Review these work products by following the
            review procedures for design and implementation work products.
        </p>
    </li>
</ul>
<h4>
    Decide on approval criteria
</h4>
<p>
    As you enter each iteration, strive to clearly define up front how the test effort will be judged to have been
    sufficient, and on what basis that judgment will be measured. Do this by discussion with the individual or group
    responsible for making the approval decision.
</p>
<p>
    These are examples of ways to handle iteration approval:
</p>
<ul>
    <li>
        The project management team approves the iteration and assesses the testing effort by reviewing the test evaluation
        summaries.
    </li>
    <li>
        The customer approves the iteration by reviewing the test evaluation summaries.
    </li>
    <li>
        The customer approves the iteration based on the results of a demonstration that exercises a certain subset of the
        total tests. This subset of tests should be defined and agreed beforehand, preferably early in the iteration. These
        tests are treated as Formal-External and are often referred to as <i>acceptance tests</i>.
    </li>
    <li>
        The customer approves the system quality by conducting their own independent tests, instead. Again, the nature of
        these tests should be clearly defined and agreed beforehand, preferably early in the iteration. These tests are
        also treated as <b>Formal-External</b> and are often referred to as <i>acceptance tests.</i>
    </li>
</ul>
<p>
    This is an important decision. You cannot reach a goal if you don't know what it is.
</p></td>
</tr>
</table>
</div>
<table class="copyright" border="0" cellspacing="0" cellpadding="0">
<tr>
<td class="copyright"><p>
    Licensed Materials - Property of IBM<br />
    &copy; &nbsp;Copyright IBM Corp.&nbsp;1987, 2011.&nbsp; All Rights Reserved.
</p></td>
</tr>
</table>
</td>
</tr>
</table>
</body>
<script type="text/javascript" language="JavaScript">
				contentPage.onload();
			</script>
</html>
